# Ansible Playbooks for K3s Cluster

This directory contains Ansible playbooks and roles for installing and configuring K3s on the provisioned VMs.

## Prerequisites

1. VMs provisioned via Terraform
2. Ansible >= 2.15 installed
3. SSH access to all nodes
4. Inventory file generated by Terraform

## Structure

```
ansible/
├── inventory.ini          # Generated by Terraform
├── group_vars/
│   └── all.yml           # Cluster-wide variables
├── roles/
│   ├── k3s-install/      # K3s installation
│   ├── nfs-client/       # NFS provisioner setup
│   └── storage-classes/  # StorageClass definitions
└── playbooks/
    ├── site.yml          # Main playbook
    ├── 01-prerequisites.yml
    ├── 02-k3s-cluster.yml
    ├── 02a-setup-storage-node.yml
    ├── 03-storage.yml
    ├── 04-gpu-operator.yml
    └── 05-rancher.yml
```

## Configuration

### 1. Review inventory

The inventory file is auto-generated by Terraform at `inventory.ini`.

### 2. Configure variables

Edit `group_vars/all.yml` to customize:
- K3s version
- Cluster configuration
- NFS server details
- Storage paths

## Usage

### Full deployment

```bash
ansible-playbook -i inventory.ini playbooks/site.yml
```

### Individual steps

#### 1. Prerequisites (system updates, dependencies)
```bash
ansible-playbook -i inventory.ini playbooks/01-prerequisites.yml
```

#### 2. K3s cluster installation
```bash
ansible-playbook -i inventory.ini playbooks/02-k3s-cluster.yml
```

#### 3. Storage configuration
```bash
ansible-playbook -i inventory.ini playbooks/03-storage.yml
```

#### 4. GPU Operator installation
```bash
ansible-playbook -i inventory.ini playbooks/04-gpu-operator.yml
```

#### 5. Rancher installation with OIDC (Authentik)
Assumptions:
- DNS `rancher_hostname` points to your cluster ingress (e.g., a control-plane IP).
- Self-signed TLS via cert-manager is acceptable. Adjust to Let's Encrypt as needed.

Configure variables in `group_vars/all.yml` under `Rancher configuration` and `rancher_oidc`.

Run:
```bash
ansible-playbook -i inventory.ini playbooks/05-rancher.yml
```

After deployment:
- Open https://<rancher_hostname>
- Login with bootstrap password (`rancher_bootstrap_password`) on first visit
- Verify OIDC config under Global Settings > Authentication; enable if not already active.

## What Gets Installed

### Control Plane Nodes
- K3s server with embedded etcd (HA mode)
- Taints: `node-role.kubernetes.io/control-plane=true:NoSchedule`
- Labels: `node-role.kubernetes.io/control-plane=true`

### GPU Worker Nodes
- K3s agent
- Node labels:
  - `accelerator=nvidia`
  - `scratch=nvme`
  - `gpu-model=a100`
- NVIDIA Container Toolkit prerequisites
- NVMe scratch mount at `/mnt/nvme/scratch`

### Storage
- NFS Subdir External Provisioner (default StorageClass)
- Local-path provisioner for fast scratch storage

### Add-ons
- Helm (if not present)
- kubectl (if not present)
- NVIDIA GPU Operator via Helm

## Verification

After installation, verify the cluster:

```bash
# Get kubeconfig from first control plane
scp ubuntu@10.0.0.11:/etc/rancher/k3s/k3s.yaml ./kubeconfig
sed -i 's/127.0.0.1/10.0.0.11/g' kubeconfig
export KUBECONFIG=./kubeconfig

# Check nodes
kubectl get nodes -o wide

# Check GPU operator pods
kubectl get pods -n gpu-operator

# Check storage classes
kubectl get sc

# Test GPU
kubectl apply -f ../test-manifests/cuda-vectoradd.yaml
kubectl logs cuda-vectoradd
```

## Troubleshooting

### K3s won't start
- Check systemd logs: `journalctl -u k3s -f`
- Verify network connectivity between nodes
- Check firewall rules (6443, 10250, 2379-2380)

### GPU not detected
- Verify GPUs visible in VM: `lspci | grep NVIDIA`
- Check GPU Operator logs: `kubectl logs -n gpu-operator -l app=nvidia-device-plugin-daemonset`

### Storage issues
- Verify NFS mount: `showmount -e 10.0.0.30`
- Check provisioner logs: `kubectl logs -n kube-system -l app=nfs-subdir-external-provisioner`

## Next Steps

After cluster is ready:
1. Install Rancher (see [../../cluster-maintenance/README.md](../../cluster-maintenance/README.md))
2. Configure Fleet GitOps
3. Deploy JupyterHub
