# GPU Test Pods - Run manually for testing
# These test pods are not managed by Fleet to avoid conflicts
# with manually created test instances.
#
# To run these tests manually:
# kubectl apply -f cuda-tests.yaml
# kubectl delete -f cuda-tests.yaml (to clean up)
#
# Tests:
# 1. cuda-vectoradd - NVIDIA CUDA vector addition sample
# 2. gpu-test-pytorch - PyTorch GPU detection and computation test

# apiVersion: v1
# kind: Pod
# metadata:
#   name: cuda-vectoradd
#   namespace: default
# spec:
#   restartPolicy: OnFailure
#   nodeSelector:
#     accelerator: nvidia
#   containers:
#   - name: cuda-vectoradd
#     image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1"
#     resources:
#       limits:
#         nvidia.com/gpu: 1
#       requests:
#         nvidia.com/gpu: 1
# ---
# apiVersion: v1
# kind: Pod
# metadata:
#   name: gpu-test-pytorch
#   namespace: default
# spec:
#   restartPolicy: OnFailure
#   nodeSelector:
#     accelerator: nvidia
#   containers:
#   - name: pytorch
#     image: nvcr.io/nvidia/pytorch:23.10-py3
#     command:
#       - python3
#       - -c
#       - |
#         import torch
#         print(f"PyTorch version: {torch.__version__}")
#         print(f"CUDA available: {torch.cuda.is_available()}")
#         print(f"CUDA version: {torch.version.cuda}")
#         print(f"GPU count: {torch.cuda.device_count()}")
#         if torch.cuda.is_available():
#             for i in range(torch.cuda.device_count()):
#                 print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
#                 print(f"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
#             # Simple computation test
#             x = torch.rand(10000, 10000).cuda()
#             y = torch.rand(10000, 10000).cuda()
#             z = x @ y
#             print(f"Successfully performed matrix multiplication on GPU: {z.shape}")
#         else:
#             print("ERROR: CUDA not available!")
#     resources:
#       limits:
#         nvidia.com/gpu: 1
#       requests:
#         nvidia.com/gpu: 1
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
---
apiVersion: v1
kind: Pod
metadata:
  name: dcgm-exporter-test
  namespace: default
spec:
  restartPolicy: OnFailure
  nodeSelector:
    accelerator: nvidia
  containers:
  - name: dcgm-query
    image: nvidia/dcgm:3.2.5-1-ubuntu22.04
    command:
      - /bin/bash
      - -c
      - |
        dcgmi discovery -l
        dcgmi dmon -e 155,204,203 -c 5
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
