# =============================================================================
# JupyterHub Configuration for GPU/CPU Cluster with Fast-Default Homes
# =============================================================================

proxy:
  service:
    type: ClusterIP
  chp:
    # More workers for the proxy under higher concurrency
    extraEnv:
      CONFIGPROXY_NUM_WORKERS: "6"

ingress:
  enabled: true
  ingressClassName: nginx #traefik
  hosts: [jupyterhub.cps.unileoben.ac.at]
  annotations:
    kubernetes.io/ingress.class: nginx #traefik
    cert-manager.io/cluster-issuer: letsencrypt-prod
  tls:
    - secretName: jupyterhub-tls
      hosts: [jupyterhub.cps.unileoben.ac.at]

hub:
  config:
    JupyterHub:
      admin_access: true
      authenticator_class: oauthenticator.generic.GenericOAuthenticator
      allow_named_servers: true
      named_server_limit_per_user: 3
      template_vars:
        org:
          name: "MontanuniversitÃ¤t Leoben"
          logo_url: "https://www.unileoben.ac.at/fileadmin/shares/allgemeine_datenbank/logos/1-mul-logo/2-mul-logo-smoke-quer.svg"
          url: "https://www.unileoben.ac.at"

    GenericOAuthenticator:
      client_id: "vUhzKqEF0UxPtZNM8aRbA1ncaehhIAIA2x9r83FI"
      client_secret: "EbAzlZLERPQzmF2EQByhihKuUqp36u138fYERPptymppmJbWhquI4sHu9vchqtnMRqbAVnZS6nOA6G0FescWa13MOLdlegQB3yyZSqe5V32NtYsnfDOndyZHiqiL2Bj6"
      oauth_callback_url: "https://jupyterhub.cps.unileoben.ac.at/hub/oauth_callback"
      authorize_url: "https://auth.cps.unileoben.ac.at/application/o/authorize/"
      token_url: "https://auth.cps.unileoben.ac.at/application/o/token/"
      userdata_url: "https://auth.cps.unileoben.ac.at/application/o/userinfo/"
      login_service: "CPS Authentik"
      username_claim: "preferred_username"
      scope: ["openid","profile","email","groups"]
      claim_groups_key: "groups"
      manage_groups: true
      admin_groups: ["jupyter_admin"]

  # ---- Hub DB: PostgreSQL instead of sqlite ----
  db:
    type: postgres
    url: postgresql://jhub:REPLACE_ME_STRONG_PW@postgresql.jhub.svc.cluster.local:5432/jhub

  # Hub resource sizing and spawn behavior for >200 users
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

  extraEnv:
    # Larger page size reduces API pagination churn during mass-spawns
    JUPYTERHUB_API_PAGE_SIZE: "200"
    # Let quotas (if any) enforce limits, not Hub:
    JUPYTERHUB_ACTIVE_SERVER_LIMIT: "0"

  extraVolumes:
    - name: custom-templates
      configMap:
        name: jupyterhub-custom-templates
  extraVolumeMounts:
    - name: custom-templates
      mountPath: /etc/jupyterhub/custom

  extraConfig:
    00-install-oauthenticator: |
      import subprocess, sys
      try:
          import oauthenticator  # noqa
      except ImportError:
          subprocess.check_call([sys.executable, "-m", "pip", "install", "oauthenticator"])

    01-info-endpoint: |
      c.JupyterHub.template_paths = ['/etc/jupyterhub/custom']
      from jupyterhub.handlers import BaseHandler
      from tornado import web
      class InfoHandler(BaseHandler):
          @web.authenticated
          async def get(self):
              user = await self.get_current_user()
              jinja_env = self.settings.get('jinja2_env', None)
              template_vars = jinja_env.globals.get('template_vars', {}) if jinja_env else {}
              org = template_vars.get('org', {})
              html = self.render_template("info.html", user=user, base_url=self.base_url, org=org)
              self.write(html)
      c.JupyterHub.extra_handlers = [(r"/info", InfoHandler)]

    # Chainable pre_spawn_hook placeholder for future tweaks (e.g., power-user size hints)
    02-pre-spawn-chain: |
      old_hook = getattr(c.KubeSpawner, "pre_spawn_hook", None)
      async def pre_spawn_chain(spawner):
          if old_hook:
              await old_hook(spawner)
          # (reserved for future per-user tweaks)
      c.KubeSpawner.pre_spawn_hook = pre_spawn_chain

# =============================================================================
# SINGLEUSER: Storage layout
#   - $HOME (fast): emptyDir on node SSD (ephemeral)
#   - /home/jovyan/Persist: per-user NFS (dynamic PVC via nfs-subdir-provisioner)
#   - /home/jovyan/Shared: shared NFS (RWX PVC you create once)
# =============================================================================
singleuser:
  image:
    # Choose your base (CPU or CUDA variant). Leave tag pinned for reproducibility.
    name: quay.io/jupyter/datascience-notebook
    tag: "2025-11-06"
    pullPolicy: IfNotPresent

  # Use NVIDIA runtime when you select GPU profiles (kept here for completeness)
  extraPodConfig:
    runtimeClassName: nvidia

  # ---------- FAST DEFAULT HOME on SSD ----------
  storage:
    type: none                  # disable default PVC at /home
    extraVolumes:
      - name: fast-home
        emptyDir:
          # Optionally cap size; omit to allow node SSD to be fully used by pod
          # sizeLimit: 150Gi
          medium: ""            # node filesystem (SSD pool)
    extraVolumeMounts:
      - name: fast-home
        mountPath: /home/jovyan

    # ---------- Per-user durable (NFS subdir provisioner) ----------
    extraVolumeClaimTemplates:
      - name: user-persist
        mountPath: /home/jovyan/Persist
        pvcNameTemplate: jhub-persist-{username}
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: nfs-client
        resources:
          requests:
            storage: 50Gi          # bump specific users by patching their PVC or via profile variants

  # ---------- Shared RWX NFS (create PVC jhub-shared-rwx once) ----------
  extraVolumes:
    - name: shared-rwx
      persistentVolumeClaim:
        claimName: jhub-shared-rwx
  extraVolumeMounts:
    - name: shared-rwx
      mountPath: /home/jovyan/Shared
      readOnly: false

  # Keep NFS cooperative writes (group-writable)
  uid: 1000
  fsGid: 100

  # Make the fast home the natural default & keep caches local (fast)
  extraEnv:
    GRANT_SUDO: "yes"
    NOTEBOOK_ARGS: "--allow-root"

    # Open JupyterLab at $HOME (fast)
    JUPYTERHUB_SINGLEUSER_APP: "jupyter_server.serverapp.ServerApp"
    JUPYTERHUB_SINGLEUSER_ARGS: >-
      --ServerApp.default_url=/lab/tree
      --ServerApp.terminado_settings='{"shell_command":["/bin/bash"]}'

    # Put heavy caches on fast storage
    XDG_CACHE_HOME: /home/jovyan/.cache
    PIP_CACHE_DIR: /home/jovyan/.cache/pip
    HF_HOME: /home/jovyan/.cache/huggingface
    TRANSFORMERS_CACHE: /home/jovyan/.cache/huggingface/transformers
    TORCH_HOME: /home/jovyan/.cache/torch
    NUMBA_CACHE_DIR: /home/jovyan/.cache/numba

  # Non-root policy friendly cloud-metadata protection (no sidecar)
  cloudMetadata:
    blockWithIptables: true

  # Nice UX + safety net: banner & best-effort sync on graceful shutdown
  extraContainerConfig:
    lifecycle:
      postStart:
        exec:
          command:
            - /bin/sh
            - -c
            - |
              set -eu
              echo "âš¡ Fast workspace (ephemeral):   /home/jovyan" > /home/jovyan/README.txt
              echo "ðŸ’¾ Persistent storage (NFS):     /home/jovyan/Persist" >> /home/jovyan/README.txt
              echo "ðŸ‘¥ Shared storage (NFS, RWX):    /home/jovyan/Shared"  >> /home/jovyan/README.txt
              [ -e /home/jovyan/Projects ] || mkdir -p /home/jovyan/Projects
              [ -e /home/jovyan/Data ] || mkdir -p /home/jovyan/Data
              [ -e /home/jovyan/Save ] || ln -s /home/jovyan/Persist /home/jovyan/Save
              # Optional: a minimal Lab user-settings drop-in could be added here
      preStop:
        exec:
          command:
            - /bin/sh
            - -c
            - |
              # Best-effort sync (graceful stops only) of recent work to Persist/last_session
              set -eu
              dst="/home/jovyan/Persist/last_session/$(date +%Y%m%d-%H%M%S)"
              mkdir -p "$dst" || true
              rsync -a --prune-empty-dirs \
                --include='*/' \
                --include='*.ipynb' --include='*.py' --include='*.md' \
                --exclude='.cache' --exclude='**/.git' --exclude='**/__pycache__' \
                /home/jovyan/ "$dst" || true

  # Baseline CPU/RAM for non-GPU default spawns (override via profiles if used)
  cpu:
    limit: 2
    guarantee: 1
  memory:
    limit: 4G
    guarantee: 2G

# =============================================================================
# Scheduling / scale knobs for high concurrency
# =============================================================================
scheduling:
  userScheduler:
    enabled: true
  podPriority:
    enabled: true
  userPlaceholder:
    enabled: false

# Keep popular images warm on nodes to speed mass spawns
prePuller:
  continuous:
    enabled: true

# =============================================================================
# Global culler (idle-based). Per-profile max-age culler can be added in hub.extraConfig.
# =============================================================================
cull:
  enabled: true
  timeout: 3600     # 1h of no kernel activity & no websocket (if cull-connected=True)
  every: 600
  maxAge: 0
  users: false
  adminUsers: true
  removeNamedServers: false
  concurrency: 20
