# NVIDIA GPU Operator Configuration for Ubuntu 24.04

# Driver configuration
driver:
  enabled: true
  repository: nvcr.io/nvidia
  image: driver
  version: "580.95.05"
  # Disable precompiled to use generic Ubuntu 24.04 driver
  usePrecompiled: false
  # Use auto kernel module type
  kernelModuleType: "auto"

# NVIDIA Container Toolkit - K3s configuration
toolkit:
  enabled: true
  version: "v1.18.0"
  env:
    - name: CONTAINERD_CONFIG
      value: /var/lib/rancher/k3s/agent/etc/containerd/config.toml
    - name: CONTAINERD_SOCKET
      value: /run/k3s/containerd/containerd.sock
    - name: CONTAINERD_RUNTIME_CLASS
      value: nvidia
    - name: CONTAINERD_SET_AS_DEFAULT
      value: "true"

# Device Plugin
devicePlugin:
  enabled: true
  version: "v0.18.0"
  # Time-slicing configuration to expose both MIG slices and "full GPU" via replicas
  config:
    create: true
    name: device-plugin-config
    default: mig-with-replicas
    data:
      mig-with-replicas: |-
        version: v1
        flags:
          migStrategy: mixed
        sharing:
          timeSlicing:
            resources:
              # Create virtual full-GPU resources from MIG slices via replicas
              # Each 1g.5gb slice can be "overcommitted" to appear as multiple schedulable units
              - name: nvidia.com/mig-1g.5gb
                replicas: 1
              # Expose a synthetic "gpu" resource by replicating MIG slices
              # This allows scheduling "full GPU" workloads on the MIG node
              # Replicas=2 means each physical MIG slice presents as 2 logical GPUs for time-sharing
              - name: nvidia.com/mig-2g.10gb
                replicas: 1
            # Note: Actual full GPUs on non-MIG nodes remain nvidia.com/gpu without replicas

# DCGM Exporter (GPU metrics)
dcgmExporter:
  enabled: true
  version: "4.4.1-4.6.0-distroless"

# GPU Feature Discovery
gfd:
  enabled: true
  version: "v0.18.0"

# MIG scheduling strategy for device plugin: allow both full GPU and MIG resources
mig:
  # Advertise both full GPU (on non-MIG nodes) and MIG resources only on the single MIG-enabled node.
  # Strategy 'mixed' is still appropriate globally; nodes without MIG partitions will just expose nvidia.com/gpu.
  strategy: mixed

# MIG Manager (for A100 MIG partitioning - enabled by default in v25.10.0)
migManager:
  enabled: true
  # Provide a MIG partitioning ConfigMap inline. Create both 40GB and 80GB profiles; set default to one.
  config:
    create: true
    name: custom-mig-parted-configs
    # Set default to all-disabled so ALL nodes start with no MIG partitions.
    default: all-disabled
    data:
      config.yaml: |-
        version: v1
        mig-configs:
          all-disabled:
            - devices: all
              mig-enabled: false
          # Mixed profile for a single node with 2x A100 40GB (GPU0 full, GPU1 8 slices 1g.5gb)
          mixed-one-node-40gb-small:
            - devices: [0]
              mig-enabled: false
            - devices: [1]
              mig-enabled: true
              mig-devices:
                "1g.5gb": 7
          # (Optional) Alternate for future 80GB hardware
          mixed-one-node-80gb-small:
            - devices: [0]
              mig-enabled: false
            - devices: [1]
              mig-enabled: true
              mig-devices:
                "1g.10gb": 8
          # Enable MIG on BOTH GPUs (index 0 and 1) on a single node (A100 40GB: 7Ã— 1g.5gb per GPU)
          both-mig-40gb-small:
            - devices: [0]
              mig-enabled: true
              mig-devices:
                "1g.5gb": 7
            - devices: [1]
              mig-enabled: true
              mig-devices:
                "1g.5gb": 7

  # Per-node activation:
  # Label exactly ONE GPU node to enable MIG using:
  #   kubectl label node <node-name> nvidia.com/mig.config=mixed-one-node-40gb-small --overwrite
  # Leave other nodes without that label (they will use 'all-disabled').
  # Remove MIG from node:
  #   kubectl label node <node-name> nvidia.com/mig.config=all-disabled --overwrite
  # Scheduling notes on the MIG-enabled node:
  # - Full GPU resource: nvidia.com/gpu (from GPU0)
  # - MIG slice resources: nvidia.com/mig-1g.5gb (8 allocatable)
  # Requests examples:
  #   limits:
  #     nvidia.com/gpu: 1        # Full GPU (any non-MIG node or GPU0 on MIG node)
  #   OR
  #     nvidia.com/mig-1g.5gb: 1 # Single smallest slice (on MIG-enabled node only)

# Node Feature Discovery
nfd:
  enabled: true

# Operator configuration
operator:
  defaultRuntime: containerd
  initContainer:
    image: cuda
    repository: nvcr.io/nvidia
    version: 13.0.1-base-ubi9

# Tolerations for control plane if needed
tolerations: []
  # - key: "node-role.kubernetes.io/control-plane"
  #   operator: "Exists"
  #   effect: "NoSchedule"

# Node selector - deploy only to GPU nodes
nodeSelector:
  accelerator: nvidia
