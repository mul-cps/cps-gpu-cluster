{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bf946d",
   "metadata": {},
   "source": [
    "# CPS GPU Cluster Test and Verification\n",
    "\n",
    "This notebook verifies the functionality of the CPS GPU cluster after setting up OIDC authentication with Authentik.\n",
    "\n",
    "## Cluster Configuration\n",
    "\n",
    "- **JupyterHub**: v4.3.1 with Authentik OIDC authentication\n",
    "- **GPU Operator**: v25.10.0 with Ubuntu 24.04 drivers\n",
    "- **Available GPUs**: 8 GPUs total (2 per worker node across 4 nodes)\n",
    "- **Storage**: NFS-backed persistent volumes\n",
    "- **Access URL**: https://jupyterhub.cps.unileoben.ac.at\n",
    "\n",
    "## Authentication Setup\n",
    "\n",
    "- **Provider**: CPS Authentik (https://auth.cps.unileoben.ac.at)\n",
    "- **Access Groups**: cps-users (regular users), cps-admins (administrators)\n",
    "- **Profile Options**: CPU-only, Single GPU, Dual GPU, Research (4 GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c447",
   "metadata": {},
   "source": [
    "## 1. Environment and Authentication Info\n",
    "\n",
    "First, let's verify the current user and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1659e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== CPS GPU Cluster Environment ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"Username: {getpass.getuser()}\")\n",
    "print(f\"Hostname: {platform.node()}\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check if running in JupyterHub\n",
    "if 'JUPYTERHUB_USER' in os.environ:\n",
    "    print(f\"JupyterHub User: {os.environ['JUPYTERHUB_USER']}\")\n",
    "    print(f\"JupyterHub Service: {os.environ.get('JUPYTERHUB_SERVICE_NAME', 'N/A')}\")\n",
    "    \n",
    "# Check for NVIDIA environment variables\n",
    "print(f\"\\nNVIDIA Environment:\")\n",
    "print(f\"NVIDIA_VISIBLE_DEVICES: {os.environ.get('NVIDIA_VISIBLE_DEVICES', 'Not Set')}\")\n",
    "print(f\"NVIDIA_DRIVER_CAPABILITIES: {os.environ.get('NVIDIA_DRIVER_CAPABILITIES', 'Not Set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfafc2",
   "metadata": {},
   "source": [
    "## 2. GPU Detection and NVIDIA Driver Verification\n",
    "\n",
    "Check if GPUs are available and the NVIDIA drivers are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run a shell command and return the output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        return result.stdout.strip(), result.stderr.strip(), result.returncode\n",
    "    except Exception as e:\n",
    "        return \"\", str(e), 1\n",
    "\n",
    "print(\"=== GPU Hardware Detection ===\")\n",
    "\n",
    "# Check nvidia-smi\n",
    "stdout, stderr, code = run_command(\"nvidia-smi\")\n",
    "if code == 0:\n",
    "    print(\"‚úÖ nvidia-smi command successful\")\n",
    "    print(stdout)\n",
    "else:\n",
    "    print(\"‚ùå nvidia-smi failed:\")\n",
    "    print(stderr)\n",
    "\n",
    "print(\"\\n=== NVIDIA Driver Version ===\")\n",
    "stdout, stderr, code = run_command(\"nvidia-smi --query-gpu=driver_version --format=csv,noheader\")\n",
    "if code == 0:\n",
    "    print(f\"Driver Version: {stdout}\")\n",
    "else:\n",
    "    print(f\"Could not get driver version: {stderr}\")\n",
    "\n",
    "print(\"\\n=== GPU Count and Models ===\")\n",
    "stdout, stderr, code = run_command(\"nvidia-smi --query-gpu=count,name --format=csv\")\n",
    "if code == 0:\n",
    "    print(stdout)\n",
    "else:\n",
    "    print(f\"Could not get GPU info: {stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed4430",
   "metadata": {},
   "source": [
    "## 3. PyTorch GPU Test\n",
    "\n",
    "Test PyTorch integration with the available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be838c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(\"‚úÖ PyTorch imported successfully\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ CUDA is available\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        # List all available devices\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name}\")\n",
    "            print(f\"    - Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "            print(f\"    - Compute Capability: {props.major}.{props.minor}\")\n",
    "        \n",
    "        # Test GPU computation\n",
    "        print(\"\\n=== GPU Computation Test ===\")\n",
    "        device = torch.device('cuda:0')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Create test tensors\n",
    "        x = torch.randn(1000, 1000).to(device)\n",
    "        y = torch.randn(1000, 1000).to(device)\n",
    "        \n",
    "        # Perform matrix multiplication\n",
    "        start_time = torch.cuda.Event(enable_timing=True)\n",
    "        end_time = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start_time.record()\n",
    "        result = torch.mm(x, y)\n",
    "        end_time.record()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time = start_time.elapsed_time(end_time)\n",
    "        \n",
    "        print(f\"‚úÖ Matrix multiplication (1000x1000) completed in {elapsed_time:.2f}ms\")\n",
    "        print(f\"Result shape: {result.shape}\")\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå CUDA is not available\")\n",
    "        print(\"Running on CPU only\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not installed, installing...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torch\"], check=True)\n",
    "    print(\"‚úÖ PyTorch installed, please restart kernel and run again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d79f86",
   "metadata": {},
   "source": [
    "## 4. Storage Persistence Test\n",
    "\n",
    "Test if the NFS storage is working and files persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9289bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== Storage Persistence Test ===\")\n",
    "\n",
    "# Create test directory\n",
    "test_dir = Path.home() / \"cluster_test\"\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create test file with timestamp\n",
    "test_file = test_dir / \"persistence_test.json\"\n",
    "test_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"user\": getpass.getuser(),\n",
    "    \"hostname\": platform.node(),\n",
    "    \"test_type\": \"storage_persistence\"\n",
    "}\n",
    "\n",
    "# Write test file\n",
    "with open(test_file, 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Created test file: {test_file}\")\n",
    "\n",
    "# Verify file contents\n",
    "if test_file.exists():\n",
    "    with open(test_file, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    print(f\"‚úÖ File exists and contains: {loaded_data}\")\n",
    "else:\n",
    "    print(\"‚ùå Test file was not created successfully\")\n",
    "\n",
    "# Check storage space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(Path.home())\n",
    "print(f\"\\n=== Storage Information ===\")\n",
    "print(f\"Home directory: {Path.home()}\")\n",
    "print(f\"Total space: {total / 1024**3:.1f} GB\")\n",
    "print(f\"Used space: {used / 1024**3:.1f} GB\")\n",
    "print(f\"Free space: {free / 1024**3:.1f} GB\")\n",
    "\n",
    "# List previous test files\n",
    "existing_tests = list(test_dir.glob(\"*.json\"))\n",
    "if len(existing_tests) > 1:\n",
    "    print(f\"\\n‚úÖ Found {len(existing_tests)} previous test files:\")\n",
    "    for test in existing_tests:\n",
    "        print(f\"  - {test.name}\")\n",
    "else:\n",
    "    print(f\"\\nüìù This is the first test run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14683d89",
   "metadata": {},
   "source": [
    "## 5. Cluster Resource Overview\n",
    "\n",
    "Get an overview of the available cluster resources and current usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e97f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Kubernetes context if kubectl is available\n",
    "print(\"=== Kubernetes Resource Information ===\")\n",
    "\n",
    "stdout, stderr, code = run_command(\"kubectl get nodes -o wide\")\n",
    "if code == 0:\n",
    "    print(\"Cluster Nodes:\")\n",
    "    print(stdout)\n",
    "else:\n",
    "    print(\"kubectl not available in user environment (expected)\")\n",
    "\n",
    "# Check for mounted volumes\n",
    "print(\"\\n=== Mounted Filesystems ===\")\n",
    "stdout, stderr, code = run_command(\"df -h\")\n",
    "if code == 0:\n",
    "    lines = stdout.split('\\n')\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in ['nfs', 'home', 'jupyter']):\n",
    "            print(line)\n",
    "\n",
    "# System resource information\n",
    "print(f\"\\n=== Local System Resources ===\")\n",
    "stdout, stderr, code = run_command(\"lscpu | grep -E '(Model name|CPU\\\\(s\\\\)|Thread|Core)'\")\n",
    "if code == 0:\n",
    "    print(\"CPU Information:\")\n",
    "    print(stdout)\n",
    "\n",
    "stdout, stderr, code = run_command(\"free -h\")\n",
    "if code == 0:\n",
    "    print(\"\\nMemory Information:\")\n",
    "    print(stdout)\n",
    "\n",
    "# Check if this is running in a GPU-enabled pod\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n=== GPU Resource Summary ===\")\n",
    "    print(f\"GPUs Available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552d4db",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook verifies that the CPS GPU cluster is functioning correctly with:\n",
    "\n",
    "1. ‚úÖ **Authentication**: OIDC integration with Authentik\n",
    "2. ‚úÖ **GPU Access**: NVIDIA drivers and CUDA support\n",
    "3. ‚úÖ **Compute**: PyTorch GPU acceleration\n",
    "4. ‚úÖ **Storage**: Persistent NFS-backed volumes\n",
    "5. ‚úÖ **Profiles**: Multiple resource configurations available\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Test different profile options (CPU-only, Single GPU, Dual GPU, Research)\n",
    "- Run actual ML workloads to verify performance\n",
    "- Test collaborative features with multiple users\n",
    "- Verify data persistence across pod restarts\n",
    "\n",
    "### Support\n",
    "\n",
    "For issues or questions, contact the CPS infrastructure team."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
